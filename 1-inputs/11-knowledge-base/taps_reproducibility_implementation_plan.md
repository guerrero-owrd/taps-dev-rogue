# TAPs Reproducibility Implementation Plan

## 1. Purpose

This document outlines the reproducibility architecture for Technical Assistance Packages (TAPs) following the five-phase analytical sequence: **Explore → Summarize → Analyze → Visualize → Interpret**.  
The objective is to maximize automation in early analytical phases while preserving human interpretive control in final reporting.

---

## 2. Design Principle

Reproducibility decreases gracefully as narrative complexity increases.  
Automation governs data acquisition, processing, and summary statistics. Human review governs meaning and context.

| Phase | Output | Automation | Review Type |
|--------|---------|-------------|-------------|
| Explore | Data completeness, QA flags | 95% | Technical QA |
| Summarize | Descriptive statistics, coverage diagnostics | 90% | Technical QA |
| Analyze | Derived indicators tied to planning criteria | 75% | Analytical review |
| Visualize | Draft plots/tables for consistency | 60% | Editorial review |
| Interpret | Basin narrative & planning context | 20–30% | Policy/Context review |

---

## 3. Workflow Structure

Each TAP component (Surface Water, Groundwater, Water Rights) will maintain a uniform folder structure:

```json

taps_component/
├── 1_explore/
│   ├── 01_data_availability.R
│   ├── 02_data_gaps.html
├── 2_summarize/
│   ├── 01_summary_stats.R
│   ├── 02_coverage_tables.csv
├── 3_analyze/
│   ├── 01_indicator_metrics.R
│   ├── 02_results.csv
├── 4_visualize/
│   ├── 01_plot_functions.R
│   ├── 02_figures/
├── 5_interpret/
│   ├── 01_tap_narrative.qmd
│   ├── 02_appendices.qmd
```

Each phase writes a **metadata.json** file with standard fields:

```json
{
  "data_version": "SWARS_2025Q2",
  "run_date": "2025-10-20",
  "author": "TAPs Team",
  "status": "analyzed"
}
```

---

## 4. Review Architecture

| Layer | Description | Review Audience |
|--------|--------------|----------------|
| (a) Exploratory Summary (“Stats CV”) | Raw indicators & QA coverage | Internal analysts |
| (b) Technical Report (“Analytical Layer”) | Standardized metrics & figures | Technical reviewers |
| (c) Interpretive TAP (“Public Layer”) | Human-readable synthesis | Planning groups |

Reviews proceed in reverse order of complexity: **Narrative → Technical Report → Code.**

---

## 5. Integration With Governance

This structure supports charter requirements for reproducibility and internal review:  

- **Technical Review:** phases (a)+(b) — assess accuracy and method consistency.  
- **Usability Review:** phase (c) — assess clarity and relevance to planning groups.  
- **Non-regulatory boundary:** all TAP outputs carry the *“Illustrative — not for enforcement”* banner.

---

## 6. Version Control and Automation

- Each TAP generation run creates a tagged commit (`TAPv_x.y.z`).
- Metadata stored in `/3-outputs/31-product-store/` by component.
- Containerization or `targets`-based workflow optional but recommended for reproducibility at scale.

---

## 7. Efficiency Benefits

- **Frontloaded automation**: Early scripts handle data retrieval and summary, reducing analyst time per basin.
- **Layered QA**: Reviewers focus on their domain layer instead of raw code.
- **Traceability**: Every figure/table is linked to a commit hash and data version.
- **Scalability**: New basins can be generated by calling a single run command (`run_basin("HUC10")`).

---

## 8. Summary

This architecture ensures that TAPs remain reproducible, modular, and reviewable.  
It balances efficiency with transparency by defining clear roles for automation, technical analysis, and human interpretation.
